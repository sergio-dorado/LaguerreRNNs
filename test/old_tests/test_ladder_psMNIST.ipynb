{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from architectures_v1 import *\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, RNN\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Training/Testing/Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train: (50000, 784, 1) - y_train: (50000,)\n",
      "x_test: (10000, 784, 1) - y_test: (10000,)\n",
      "x_valid: (10000, 784, 1) - y_valid: (10000,)\n"
     ]
    }
   ],
   "source": [
    "dataset = \"PMNIST\"\n",
    "\n",
    "x_train_load = np.load(os.path.abspath(os.path.join('99_data', dataset,'x_train.npz')))\n",
    "x_test_load = np.load(os.path.abspath(os.path.join('99_data', dataset, 'x_test.npz')))\n",
    "x_valid_load = np.load(os.path.abspath(os.path.join('99_data', dataset,'x_valid.npz')))\n",
    "\n",
    "x_train = x_train_load['arr_0']\n",
    "x_test = x_test_load['arr_0']\n",
    "x_valid = x_valid_load['arr_0']\n",
    "\n",
    "y_train_load = np.load(os.path.abspath(os.path.join('99_data', dataset,'y_train.npz')))\n",
    "y_test_load = np.load(os.path.abspath(os.path.join('99_data', dataset,'y_test.npz')))\n",
    "y_valid_load = np.load(os.path.abspath(os.path.join('99_data', dataset,'y_valid.npz')))\n",
    "\n",
    "y_train = y_train_load['arr_0']\n",
    "y_test = y_test_load['arr_0']\n",
    "y_valid = y_valid_load['arr_0']\n",
    "\n",
    "print(f\"x_train: {x_train.shape} - y_train: {y_train.shape}\")\n",
    "print(f\"x_test: {x_test.shape} - y_test: {y_test.shape}\")\n",
    "print(f\"x_valid: {x_valid.shape} - y_valid: {y_valid.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\sergio\\.conda\\envs\\sd_dev_tf_1\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "rnn (RNN)                    (None, 212)               826022    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 10)                2130      \n",
      "=================================================================\n",
      "Total params: 828,152\n",
      "Trainable params: 213,495\n",
      "Non-trainable params: 614,657\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "seed = 42\n",
    "tf.random.set_random_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "length = x_train.shape[1]\n",
    "n_features = x_train.shape[-1]\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(RNN(LadderCell(units = 212,\n",
    "                             max_delay = 782,\n",
    "                             input_dims = 1), \n",
    "              input_shape = (length, n_features),\n",
    "             return_sequences = False))\n",
    "model.add(Dense(to_categorical(y_train).shape[-1], activation = \"softmax\"))\n",
    "\n",
    "model.compile(optimizer = \"adam\", loss = \"categorical_crossentropy\", metrics = [\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/200\n",
      "49900/50000 [============================>.] - ETA: 0s - loss: 0.2918 - acc: 0.9124\n",
      "Epoch 00001: val_loss improved from inf to 0.17747, saving model to C:\\Users\\Sergio\\Documents\\GitHub_Repositories\\01_Maintained\\NeuralODE\\models\\PMNIST\\PMNIST-laguerre.hdf5\n",
      "50000/50000 [==============================] - 198s 4ms/sample - loss: 0.2917 - acc: 0.9124 - val_loss: 0.1775 - val_acc: 0.9464\n",
      "Epoch 2/200\n",
      "49900/50000 [============================>.] - ETA: 0s - loss: 0.1386 - acc: 0.9593\n",
      "Epoch 00002: val_loss improved from 0.17747 to 0.14212, saving model to C:\\Users\\Sergio\\Documents\\GitHub_Repositories\\01_Maintained\\NeuralODE\\models\\PMNIST\\PMNIST-laguerre.hdf5\n",
      "50000/50000 [==============================] - 198s 4ms/sample - loss: 0.1387 - acc: 0.9593 - val_loss: 0.1421 - val_acc: 0.9572\n",
      "Epoch 3/200\n",
      "49900/50000 [============================>.] - ETA: 0s - loss: 0.1037 - acc: 0.9685\n",
      "Epoch 00003: val_loss improved from 0.14212 to 0.12710, saving model to C:\\Users\\Sergio\\Documents\\GitHub_Repositories\\01_Maintained\\NeuralODE\\models\\PMNIST\\PMNIST-laguerre.hdf5\n",
      "50000/50000 [==============================] - 198s 4ms/sample - loss: 0.1036 - acc: 0.9686 - val_loss: 0.1271 - val_acc: 0.9617\n",
      "Epoch 4/200\n",
      "49900/50000 [============================>.] - ETA: 0s - loss: 0.0815 - acc: 0.9753\n",
      "Epoch 00004: val_loss improved from 0.12710 to 0.11970, saving model to C:\\Users\\Sergio\\Documents\\GitHub_Repositories\\01_Maintained\\NeuralODE\\models\\PMNIST\\PMNIST-laguerre.hdf5\n",
      "50000/50000 [==============================] - 198s 4ms/sample - loss: 0.0815 - acc: 0.9753 - val_loss: 0.1197 - val_acc: 0.9627\n",
      "Epoch 5/200\n",
      "49900/50000 [============================>.] - ETA: 0s - loss: 0.0667 - acc: 0.9795\n",
      "Epoch 00005: val_loss improved from 0.11970 to 0.11165, saving model to C:\\Users\\Sergio\\Documents\\GitHub_Repositories\\01_Maintained\\NeuralODE\\models\\PMNIST\\PMNIST-laguerre.hdf5\n",
      "50000/50000 [==============================] - 204s 4ms/sample - loss: 0.0668 - acc: 0.9794 - val_loss: 0.1117 - val_acc: 0.9656\n",
      "Epoch 6/200\n",
      "49900/50000 [============================>.] - ETA: 0s - loss: 0.0581 - acc: 0.9818\n",
      "Epoch 00006: val_loss improved from 0.11165 to 0.10536, saving model to C:\\Users\\Sergio\\Documents\\GitHub_Repositories\\01_Maintained\\NeuralODE\\models\\PMNIST\\PMNIST-laguerre.hdf5\n",
      "50000/50000 [==============================] - 202s 4ms/sample - loss: 0.0581 - acc: 0.9818 - val_loss: 0.1054 - val_acc: 0.9698\n",
      "Epoch 7/200\n",
      "49900/50000 [============================>.] - ETA: 0s - loss: 0.0505 - acc: 0.9842\n",
      "Epoch 00007: val_loss improved from 0.10536 to 0.10440, saving model to C:\\Users\\Sergio\\Documents\\GitHub_Repositories\\01_Maintained\\NeuralODE\\models\\PMNIST\\PMNIST-laguerre.hdf5\n",
      "50000/50000 [==============================] - 198s 4ms/sample - loss: 0.0504 - acc: 0.9843 - val_loss: 0.1044 - val_acc: 0.9682\n",
      "Epoch 8/200\n",
      "49900/50000 [============================>.] - ETA: 0s - loss: 0.0469 - acc: 0.9856\n",
      "Epoch 00008: val_loss did not improve from 0.10440\n",
      "50000/50000 [==============================] - 201s 4ms/sample - loss: 0.0469 - acc: 0.9855 - val_loss: 0.1087 - val_acc: 0.9677\n",
      "Epoch 9/200\n",
      "49900/50000 [============================>.] - ETA: 0s - loss: 0.0415 - acc: 0.9875\n",
      "Epoch 00009: val_loss improved from 0.10440 to 0.10058, saving model to C:\\Users\\Sergio\\Documents\\GitHub_Repositories\\01_Maintained\\NeuralODE\\models\\PMNIST\\PMNIST-laguerre.hdf5\n",
      "50000/50000 [==============================] - 198s 4ms/sample - loss: 0.0414 - acc: 0.9875 - val_loss: 0.1006 - val_acc: 0.9686\n",
      "Epoch 10/200\n",
      "49900/50000 [============================>.] - ETA: 0s - loss: 0.0380 - acc: 0.9885\n",
      "Epoch 00010: val_loss did not improve from 0.10058\n",
      "50000/50000 [==============================] - 198s 4ms/sample - loss: 0.0381 - acc: 0.9885 - val_loss: 0.1074 - val_acc: 0.9675\n",
      "Epoch 11/200\n",
      "49900/50000 [============================>.] - ETA: 0s - loss: 0.0389 - acc: 0.9878\n",
      "Epoch 00011: val_loss did not improve from 0.10058\n",
      "50000/50000 [==============================] - 197s 4ms/sample - loss: 0.0389 - acc: 0.9878 - val_loss: 0.1076 - val_acc: 0.9678\n",
      "Epoch 12/200\n",
      "49900/50000 [============================>.] - ETA: 0s - loss: 0.0338 - acc: 0.9895\n",
      "Epoch 00012: val_loss did not improve from 0.10058\n",
      "50000/50000 [==============================] - 198s 4ms/sample - loss: 0.0339 - acc: 0.9894 - val_loss: 0.1110 - val_acc: 0.9670\n",
      "Epoch 13/200\n",
      "49900/50000 [============================>.] - ETA: 0s - loss: 0.0391 - acc: 0.9867\n",
      "Epoch 00013: val_loss did not improve from 0.10058\n",
      "50000/50000 [==============================] - 197s 4ms/sample - loss: 0.0391 - acc: 0.9867 - val_loss: 0.1105 - val_acc: 0.9666\n",
      "Epoch 14/200\n",
      "49900/50000 [============================>.] - ETA: 0s - loss: 0.0277 - acc: 0.9914\n",
      "Epoch 00014: val_loss did not improve from 0.10058\n",
      "50000/50000 [==============================] - 198s 4ms/sample - loss: 0.0277 - acc: 0.9914 - val_loss: 0.1175 - val_acc: 0.9688\n",
      "Epoch 15/200\n",
      "49900/50000 [============================>.] - ETA: 0s - loss: 0.1031 - acc: 0.9675\n",
      "Epoch 00015: val_loss did not improve from 0.10058\n",
      "50000/50000 [==============================] - 196s 4ms/sample - loss: 0.1030 - acc: 0.9676 - val_loss: 0.1561 - val_acc: 0.9543\n",
      "Epoch 16/200\n",
      "49900/50000 [============================>.] - ETA: 0s - loss: 0.0738 - acc: 0.9759\n",
      "Epoch 00016: val_loss did not improve from 0.10058\n",
      "50000/50000 [==============================] - 196s 4ms/sample - loss: 0.0738 - acc: 0.9759 - val_loss: 0.1229 - val_acc: 0.9629\n",
      "Epoch 17/200\n",
      "49900/50000 [============================>.] - ETA: 0s - loss: 0.0395 - acc: 0.9877\n",
      "Epoch 00017: val_loss did not improve from 0.10058\n",
      "50000/50000 [==============================] - 196s 4ms/sample - loss: 0.0395 - acc: 0.9876 - val_loss: 0.1101 - val_acc: 0.9672\n",
      "Epoch 18/200\n",
      "49900/50000 [============================>.] - ETA: 0s - loss: 0.0258 - acc: 0.9929\n",
      "Epoch 00018: val_loss did not improve from 0.10058\n",
      "50000/50000 [==============================] - 197s 4ms/sample - loss: 0.0258 - acc: 0.9929 - val_loss: 0.1034 - val_acc: 0.9698\n",
      "Epoch 19/200\n",
      "49900/50000 [============================>.] - ETA: 0s - loss: 0.0193 - acc: 0.9950\n",
      "Epoch 00019: val_loss did not improve from 0.10058\n",
      "50000/50000 [==============================] - 199s 4ms/sample - loss: 0.0194 - acc: 0.9950 - val_loss: 0.1060 - val_acc: 0.9714\n",
      "Training time: 3769.2709884643555 s\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(os.path.abspath(os.path.join('models', dataset))):\n",
    "    os.mkdir(os.path.abspath(os.path.join('models', dataset)))\n",
    "\n",
    "file_path = os.path.abspath(os.path.join('models', dataset, f'{dataset}-ladder.hdf5'))\n",
    "\n",
    "callbacks = [\n",
    "    ModelCheckpoint(filepath=file_path, monitor='val_loss', save_best_only = True, mode = 'min', verbose = 1), \n",
    "    EarlyStopping(monitor = 'val_loss', patience = 10, mode = 'min')]\n",
    "\n",
    "t = time.time()\n",
    "\n",
    "result = model.fit(x_train, \n",
    "                   to_categorical(y_train),\n",
    "                   epochs = 200, \n",
    "                   batch_size = 100, \n",
    "                   validation_data = (x_valid, to_categorical(y_valid)), \n",
    "                   callbacks = callbacks)\n",
    "\n",
    "print(f\"Training time: {time.time() - t} s\")\n",
    "\n",
    "df_results = pd.DataFrame(result.history)\n",
    "df_results.to_csv(os.path.abspath(os.path.join('models', dataset, 'ladder_results.csv')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 37s 4ms/sample - loss: 0.0927 - acc: 0.9729\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0927379258789122, 0.9729]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights(file_path)\n",
    "model.evaluate(x_test, to_categorical(y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
