{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from architectures_v1 import *\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, RNN\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Training/Testing/Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_all: (3877, 166, 1) - y_all: (3877,)\n",
      "x_valid: (430, 166, 1) - y_valid: (430,)\n"
     ]
    }
   ],
   "source": [
    "dataset = \"ChlorineConcentration\"\n",
    "\n",
    "x_train_load = np.load(os.path.abspath(os.path.join('99_data', dataset,'x_train.npz')))\n",
    "x_test_load = np.load(os.path.abspath(os.path.join('99_data', dataset,'x_test.npz')))\n",
    "\n",
    "x_train = np.reshape(x_train_load['arr_0'], [x_train_load['arr_0'].shape[0], x_train_load['arr_0'].shape[1], 1])\n",
    "x_test = np.reshape(x_test_load['arr_0'], [x_test_load['arr_0'].shape[0], x_test_load['arr_0'].shape[1], 1])\n",
    "\n",
    "x_all = np.concatenate((x_train, x_test), axis = 0)\n",
    "\n",
    "n_instances = x_all.shape[0]\n",
    "\n",
    "y_train_load = np.load(os.path.abspath(os.path.join('99_data', dataset,'y_train.npz')))\n",
    "y_test_load = np.load(os.path.abspath(os.path.join('99_data', dataset,'y_test.npz')))\n",
    "\n",
    "y_train = y_train_load['arr_0']\n",
    "y_test = y_test_load['arr_0']\n",
    "\n",
    "y_all = np.concatenate((y_train, y_test), axis = 0)\n",
    "y_all = np.asarray(y_all, dtype = np.uint64)\n",
    "\n",
    "x_valid = x_all[:int(0.1*n_instances), :, :]\n",
    "y_valid = y_all[:int(0.1*n_instances)]\n",
    "\n",
    "x_all = x_all[int(0.1*n_instances):, :, :]\n",
    "y_all = y_all[int(0.1*n_instances):]\n",
    "\n",
    "print(f\"x_all: {x_all.shape} - y_all: {y_all.shape}\")\n",
    "print(f\"x_valid: {x_valid.shape} - y_valid: {y_valid.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/dorads/miniconda3/envs/sd_dev_tf_1/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "rnn (RNN)                    (None, 212)               58571     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 3)                 639       \n",
      "=================================================================\n",
      "Total params: 59,210\n",
      "Trainable params: 56,658\n",
      "Non-trainable params: 2,552\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "seed = 42\n",
    "tf.random.set_random_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "length = x_train.shape[1]\n",
    "n_features = x_train.shape[-1]\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(RNN(OrthogonalCell(units = 212,\n",
    "                             order = 256,\n",
    "                             variant = 'ct_laguerre',\n",
    "                             dt = 1,\n",
    "                            input_dims = n_features), \n",
    "              input_shape = (length, n_features),\n",
    "             return_sequences = False))\n",
    "model.add(Dense(to_categorical(y_all).shape[-1], activation = \"softmax\"))\n",
    "\n",
    "model.compile(optimizer = \"adam\", loss = \"categorical_crossentropy\", metrics = [\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold: 1\n",
      "\n",
      "Train on 3101 samples, validate on 430 samples\n",
      "Epoch 1/2000\n",
      "3100/3101 [============================>.] - ETA: 0s - loss: 1.0968 - acc: 0.5210\n",
      "Epoch 00001: val_loss improved from inf to 1.01503, saving model to /data/home/dorads/Documents/GitHub_Repositories/01_Maintained/NeuralODE/models/ChlorineConcentration/ChlorineConcentration-laguerre_1.hdf5\n",
      "3101/3101 [==============================] - 3s 1ms/sample - loss: 1.0969 - acc: 0.5208 - val_loss: 1.0150 - val_acc: 0.5512\n",
      "Epoch 2/2000\n",
      "3100/3101 [============================>.] - ETA: 0s - loss: 1.0387 - acc: 0.5126\n",
      "Epoch 00002: val_loss did not improve from 1.01503\n",
      "3101/3101 [==============================] - 3s 897us/sample - loss: 1.0388 - acc: 0.5124 - val_loss: 1.0274 - val_acc: 0.5558\n",
      "Epoch 3/2000\n",
      "3100/3101 [============================>.] - ETA: 0s - loss: 1.0342 - acc: 0.5232\n",
      "Epoch 00003: val_loss did not improve from 1.01503\n",
      "3101/3101 [==============================] - 3s 902us/sample - loss: 1.0340 - acc: 0.5234 - val_loss: 1.0325 - val_acc: 0.5651\n",
      "Epoch 4/2000\n",
      "3100/3101 [============================>.] - ETA: 0s - loss: 1.0211 - acc: 0.5335\n",
      "Epoch 00004: val_loss improved from 1.01503 to 0.99650, saving model to /data/home/dorads/Documents/GitHub_Repositories/01_Maintained/NeuralODE/models/ChlorineConcentration/ChlorineConcentration-laguerre_1.hdf5\n",
      "3101/3101 [==============================] - 3s 896us/sample - loss: 1.0210 - acc: 0.5337 - val_loss: 0.9965 - val_acc: 0.5767\n",
      "Epoch 5/2000\n",
      "3100/3101 [============================>.] - ETA: 0s - loss: 1.0069 - acc: 0.5461\n",
      "Epoch 00005: val_loss improved from 0.99650 to 0.98902, saving model to /data/home/dorads/Documents/GitHub_Repositories/01_Maintained/NeuralODE/models/ChlorineConcentration/ChlorineConcentration-laguerre_1.hdf5\n",
      "3101/3101 [==============================] - 3s 902us/sample - loss: 1.0068 - acc: 0.5463 - val_loss: 0.9890 - val_acc: 0.5837\n",
      "Epoch 6/2000\n",
      "3100/3101 [============================>.] - ETA: 0s - loss: 0.9984 - acc: 0.5519\n",
      "Epoch 00006: val_loss did not improve from 0.98902\n",
      "3101/3101 [==============================] - 3s 901us/sample - loss: 0.9986 - acc: 0.5518 - val_loss: 1.0431 - val_acc: 0.4698\n",
      "Epoch 7/2000\n",
      "3100/3101 [============================>.] - ETA: 0s - loss: 1.0286 - acc: 0.5181\n",
      "Epoch 00007: val_loss did not improve from 0.98902\n",
      "3101/3101 [==============================] - 3s 930us/sample - loss: 1.0285 - acc: 0.5182 - val_loss: 0.9991 - val_acc: 0.5395\n",
      "Epoch 8/2000\n",
      "3100/3101 [============================>.] - ETA: 0s - loss: 1.0132 - acc: 0.5510\n",
      "Epoch 00008: val_loss did not improve from 0.98902\n",
      "3101/3101 [==============================] - 3s 881us/sample - loss: 1.0133 - acc: 0.5508 - val_loss: 1.0122 - val_acc: 0.5140\n",
      "Epoch 9/2000\n",
      "3100/3101 [============================>.] - ETA: 0s - loss: 1.0078 - acc: 0.5290\n",
      "Epoch 00009: val_loss improved from 0.98902 to 0.95388, saving model to /data/home/dorads/Documents/GitHub_Repositories/01_Maintained/NeuralODE/models/ChlorineConcentration/ChlorineConcentration-laguerre_1.hdf5\n",
      "3101/3101 [==============================] - 3s 929us/sample - loss: 1.0077 - acc: 0.5292 - val_loss: 0.9539 - val_acc: 0.5837\n",
      "Epoch 10/2000\n",
      "3100/3101 [============================>.] - ETA: 0s - loss: 0.9982 - acc: 0.5539\n",
      "Epoch 00010: val_loss did not improve from 0.95388\n",
      "3101/3101 [==============================] - 3s 913us/sample - loss: 0.9984 - acc: 0.5537 - val_loss: 0.9613 - val_acc: 0.5860\n",
      "Epoch 11/2000\n",
      "3100/3101 [============================>.] - ETA: 0s - loss: 0.9825 - acc: 0.5568\n",
      "Epoch 00011: val_loss did not improve from 0.95388\n",
      "3101/3101 [==============================] - 3s 915us/sample - loss: 0.9826 - acc: 0.5566 - val_loss: 0.9661 - val_acc: 0.5837\n",
      "Epoch 12/2000\n",
      "3100/3101 [============================>.] - ETA: 0s - loss: 0.9862 - acc: 0.5532\n",
      "Epoch 00012: val_loss did not improve from 0.95388\n",
      "3101/3101 [==============================] - 3s 906us/sample - loss: 0.9863 - acc: 0.5530 - val_loss: 0.9809 - val_acc: 0.5930\n",
      "Epoch 13/2000\n",
      "3100/3101 [============================>.] - ETA: 0s - loss: 0.9775 - acc: 0.5626\n",
      "Epoch 00013: val_loss improved from 0.95388 to 0.94519, saving model to /data/home/dorads/Documents/GitHub_Repositories/01_Maintained/NeuralODE/models/ChlorineConcentration/ChlorineConcentration-laguerre_1.hdf5\n",
      "3101/3101 [==============================] - 3s 905us/sample - loss: 0.9775 - acc: 0.5624 - val_loss: 0.9452 - val_acc: 0.5884\n",
      "Epoch 14/2000\n",
      "3100/3101 [============================>.] - ETA: 0s - loss: 1.0105 - acc: 0.5468\n",
      "Epoch 00014: val_loss did not improve from 0.94519\n",
      "3101/3101 [==============================] - 3s 890us/sample - loss: 1.0104 - acc: 0.5469 - val_loss: 0.9836 - val_acc: 0.5698\n",
      "Epoch 15/2000\n",
      "3100/3101 [============================>.] - ETA: 0s - loss: 0.9995 - acc: 0.5535\n",
      "Epoch 00015: val_loss did not improve from 0.94519\n",
      "3101/3101 [==============================] - 3s 903us/sample - loss: 0.9994 - acc: 0.5537 - val_loss: 0.9668 - val_acc: 0.5698\n",
      "Epoch 16/2000\n",
      "3100/3101 [============================>.] - ETA: 0s - loss: 0.9846 - acc: 0.5581\n",
      "Epoch 00016: val_loss did not improve from 0.94519\n",
      "3101/3101 [==============================] - 3s 910us/sample - loss: 0.9845 - acc: 0.5582 - val_loss: 0.9834 - val_acc: 0.5907\n",
      "Epoch 17/2000\n",
      "3100/3101 [============================>.] - ETA: 0s - loss: 0.9793 - acc: 0.5655\n",
      "Epoch 00017: val_loss did not improve from 0.94519\n",
      "3101/3101 [==============================] - 3s 895us/sample - loss: 0.9794 - acc: 0.5653 - val_loss: 0.9905 - val_acc: 0.5047\n",
      "Epoch 18/2000\n",
      "3100/3101 [============================>.] - ETA: 0s - loss: 1.0170 - acc: 0.5181\n",
      "Epoch 00018: val_loss did not improve from 0.94519\n",
      "3101/3101 [==============================] - 3s 900us/sample - loss: 1.0169 - acc: 0.5182 - val_loss: 0.9694 - val_acc: 0.5953\n",
      "Epoch 19/2000\n",
      "3100/3101 [============================>.] - ETA: 0s - loss: 0.9739 - acc: 0.5610\n",
      "Epoch 00019: val_loss did not improve from 0.94519\n",
      "3101/3101 [==============================] - 3s 894us/sample - loss: 0.9741 - acc: 0.5608 - val_loss: 0.9761 - val_acc: 0.5674\n",
      "Epoch 20/2000\n",
      "3100/3101 [============================>.] - ETA: 0s - loss: 1.0017 - acc: 0.5390\n",
      "Epoch 00020: val_loss did not improve from 0.94519\n",
      "3101/3101 [==============================] - 3s 903us/sample - loss: 1.0019 - acc: 0.5389 - val_loss: 0.9778 - val_acc: 0.5814\n",
      "Epoch 21/2000\n",
      "3100/3101 [============================>.] - ETA: 0s - loss: 0.9942 - acc: 0.5481\n",
      "Epoch 00021: val_loss did not improve from 0.94519\n",
      "3101/3101 [==============================] - 3s 909us/sample - loss: 0.9943 - acc: 0.5479 - val_loss: 0.9773 - val_acc: 0.5698\n",
      "Epoch 22/2000\n",
      "3100/3101 [============================>.] - ETA: 0s - loss: 0.9918 - acc: 0.5394\n",
      "Epoch 00022: val_loss did not improve from 0.94519\n",
      "3101/3101 [==============================] - 3s 899us/sample - loss: 0.9919 - acc: 0.5392 - val_loss: 1.0235 - val_acc: 0.5163\n",
      "Epoch 23/2000\n",
      "3100/3101 [============================>.] - ETA: 0s - loss: 0.9986 - acc: 0.5345\n",
      "Epoch 00023: val_loss did not improve from 0.94519\n",
      "3101/3101 [==============================] - 3s 900us/sample - loss: 0.9992 - acc: 0.5343 - val_loss: 0.9538 - val_acc: 0.5767\n",
      "Epoch 24/2000\n",
      "3100/3101 [============================>.] - ETA: 0s - loss: 1.0002 - acc: 0.5558\n",
      "Epoch 00024: val_loss did not improve from 0.94519\n",
      "3101/3101 [==============================] - 3s 885us/sample - loss: 1.0001 - acc: 0.5559 - val_loss: 0.9791 - val_acc: 0.5930\n",
      "Epoch 25/2000\n",
      "3100/3101 [============================>.] - ETA: 0s - loss: 1.0037 - acc: 0.5526\n",
      "Epoch 00025: val_loss did not improve from 0.94519\n",
      "3101/3101 [==============================] - 3s 897us/sample - loss: 1.0038 - acc: 0.5524 - val_loss: 1.0133 - val_acc: 0.5791\n",
      "Epoch 26/2000\n",
      "3100/3101 [============================>.] - ETA: 0s - loss: 0.9948 - acc: 0.5439\n",
      "Epoch 00026: val_loss did not improve from 0.94519\n",
      "3101/3101 [==============================] - 3s 907us/sample - loss: 0.9947 - acc: 0.5440 - val_loss: 0.9677 - val_acc: 0.5884\n",
      "Epoch 27/2000\n",
      "3100/3101 [============================>.] - ETA: 0s - loss: 0.9764 - acc: 0.5623\n",
      "Epoch 00027: val_loss improved from 0.94519 to 0.94451, saving model to /data/home/dorads/Documents/GitHub_Repositories/01_Maintained/NeuralODE/models/ChlorineConcentration/ChlorineConcentration-laguerre_1.hdf5\n",
      "3101/3101 [==============================] - 3s 911us/sample - loss: 0.9764 - acc: 0.5624 - val_loss: 0.9445 - val_acc: 0.5907\n",
      "Epoch 28/2000\n",
      "3100/3101 [============================>.] - ETA: 0s - loss: 0.9887 - acc: 0.5600\n",
      "Epoch 00028: val_loss did not improve from 0.94451\n",
      "3101/3101 [==============================] - 3s 891us/sample - loss: 0.9889 - acc: 0.5598 - val_loss: 0.9598 - val_acc: 0.5837\n",
      "Epoch 29/2000\n",
      "3100/3101 [============================>.] - ETA: 0s - loss: 0.9790 - acc: 0.5403\n",
      "Epoch 00029: val_loss did not improve from 0.94451\n",
      "3101/3101 [==============================] - 3s 894us/sample - loss: 0.9792 - acc: 0.5401 - val_loss: 0.9852 - val_acc: 0.5814\n",
      "Epoch 30/2000\n",
      "3100/3101 [============================>.] - ETA: 0s - loss: 0.9914 - acc: 0.5484\n",
      "Epoch 00030: val_loss did not improve from 0.94451\n",
      "3101/3101 [==============================] - 3s 913us/sample - loss: 0.9914 - acc: 0.5485 - val_loss: 0.9762 - val_acc: 0.5791\n",
      "Epoch 31/2000\n",
      "3100/3101 [============================>.] - ETA: 0s - loss: 0.9916 - acc: 0.5594\n",
      "Epoch 00031: val_loss did not improve from 0.94451\n",
      "3101/3101 [==============================] - 3s 909us/sample - loss: 0.9918 - acc: 0.5592 - val_loss: 0.9735 - val_acc: 0.5860\n",
      "Epoch 32/2000\n",
      "3100/3101 [============================>.] - ETA: 0s - loss: 0.9859 - acc: 0.5619\n",
      "Epoch 00032: val_loss did not improve from 0.94451\n",
      "3101/3101 [==============================] - 3s 902us/sample - loss: 0.9857 - acc: 0.5621 - val_loss: 0.9535 - val_acc: 0.5837\n",
      "Epoch 33/2000\n",
      "3100/3101 [============================>.] - ETA: 0s - loss: 0.9760 - acc: 0.5645\n",
      "Epoch 00033: val_loss did not improve from 0.94451\n",
      "3101/3101 [==============================] - 3s 889us/sample - loss: 0.9758 - acc: 0.5647 - val_loss: 0.9508 - val_acc: 0.5860\n",
      "Epoch 34/2000\n",
      "3100/3101 [============================>.] - ETA: 0s - loss: 0.9758 - acc: 0.5626\n",
      "Epoch 00034: val_loss improved from 0.94451 to 0.93933, saving model to /data/home/dorads/Documents/GitHub_Repositories/01_Maintained/NeuralODE/models/ChlorineConcentration/ChlorineConcentration-laguerre_1.hdf5\n",
      "3101/3101 [==============================] - 3s 920us/sample - loss: 0.9759 - acc: 0.5624 - val_loss: 0.9393 - val_acc: 0.5953\n",
      "Epoch 35/2000\n",
      "3100/3101 [============================>.] - ETA: 0s - loss: 0.9823 - acc: 0.5568\n",
      "Epoch 00035: val_loss improved from 0.93933 to 0.93908, saving model to /data/home/dorads/Documents/GitHub_Repositories/01_Maintained/NeuralODE/models/ChlorineConcentration/ChlorineConcentration-laguerre_1.hdf5\n",
      "3101/3101 [==============================] - 3s 919us/sample - loss: 0.9823 - acc: 0.5566 - val_loss: 0.9391 - val_acc: 0.6070\n",
      "Epoch 36/2000\n",
      "3100/3101 [============================>.] - ETA: 0s - loss: 0.9815 - acc: 0.5642\n",
      "Epoch 00036: val_loss did not improve from 0.93908\n",
      "3101/3101 [==============================] - 3s 892us/sample - loss: 0.9818 - acc: 0.5640 - val_loss: 0.9674 - val_acc: 0.6023\n",
      "Epoch 37/2000\n",
      "3100/3101 [============================>.] - ETA: 0s - loss: 0.9892 - acc: 0.5561\n",
      "Epoch 00037: val_loss did not improve from 0.93908\n",
      "3101/3101 [==============================] - 3s 907us/sample - loss: 0.9891 - acc: 0.5563 - val_loss: 0.9508 - val_acc: 0.5860\n",
      "Epoch 38/2000\n",
      "3100/3101 [============================>.] - ETA: 0s - loss: 0.9854 - acc: 0.5626\n",
      "Epoch 00038: val_loss improved from 0.93908 to 0.93803, saving model to /data/home/dorads/Documents/GitHub_Repositories/01_Maintained/NeuralODE/models/ChlorineConcentration/ChlorineConcentration-laguerre_1.hdf5\n",
      "3101/3101 [==============================] - 3s 896us/sample - loss: 0.9852 - acc: 0.5627 - val_loss: 0.9380 - val_acc: 0.5860\n",
      "Epoch 39/2000\n",
      "3100/3101 [============================>.] - ETA: 0s - loss: 0.9736 - acc: 0.5613\n",
      "Epoch 00039: val_loss did not improve from 0.93803\n",
      "3101/3101 [==============================] - 3s 898us/sample - loss: 0.9735 - acc: 0.5614 - val_loss: 0.9446 - val_acc: 0.5814\n",
      "Epoch 40/2000\n",
      "3100/3101 [============================>.] - ETA: 0s - loss: 0.9654 - acc: 0.5690\n",
      "Epoch 00040: val_loss did not improve from 0.93803\n",
      "3101/3101 [==============================] - 3s 858us/sample - loss: 0.9653 - acc: 0.5692 - val_loss: 0.9576 - val_acc: 0.5860\n",
      "Epoch 41/2000\n",
      "3100/3101 [============================>.] - ETA: 0s - loss: 0.9635 - acc: 0.5655\n",
      "Epoch 00041: val_loss did not improve from 0.93803\n",
      "3101/3101 [==============================] - 3s 899us/sample - loss: 0.9637 - acc: 0.5653 - val_loss: 0.9574 - val_acc: 0.5837\n",
      "Epoch 42/2000\n",
      "3100/3101 [============================>.] - ETA: 0s - loss: 0.9779 - acc: 0.5510\n",
      "Epoch 00042: val_loss did not improve from 0.93803\n",
      "3101/3101 [==============================] - 3s 920us/sample - loss: 0.9779 - acc: 0.5508 - val_loss: 0.9627 - val_acc: 0.5860\n",
      "Epoch 43/2000\n",
      "3100/3101 [============================>.] - ETA: 0s - loss: 0.9900 - acc: 0.5374\n",
      "Epoch 00043: val_loss did not improve from 0.93803\n",
      "3101/3101 [==============================] - 3s 906us/sample - loss: 0.9899 - acc: 0.5376 - val_loss: 0.9579 - val_acc: 0.5860\n",
      "Epoch 44/2000\n",
      "3100/3101 [============================>.] - ETA: 0s - loss: 0.9707 - acc: 0.5677\n",
      "Epoch 00044: val_loss did not improve from 0.93803\n",
      "3101/3101 [==============================] - 3s 918us/sample - loss: 0.9705 - acc: 0.5679 - val_loss: 0.9417 - val_acc: 0.5907\n",
      "Epoch 45/2000\n",
      "3100/3101 [============================>.] - ETA: 0s - loss: 0.9664 - acc: 0.5645\n",
      "Epoch 00045: val_loss did not improve from 0.93803\n",
      "3101/3101 [==============================] - 3s 873us/sample - loss: 0.9663 - acc: 0.5647 - val_loss: 0.9578 - val_acc: 0.5884\n",
      "Epoch 46/2000\n",
      "3100/3101 [============================>.] - ETA: 0s - loss: 0.9724 - acc: 0.5694\n",
      "Epoch 00046: val_loss improved from 0.93803 to 0.93512, saving model to /data/home/dorads/Documents/GitHub_Repositories/01_Maintained/NeuralODE/models/ChlorineConcentration/ChlorineConcentration-laguerre_1.hdf5\n",
      "3101/3101 [==============================] - 3s 917us/sample - loss: 0.9723 - acc: 0.5695 - val_loss: 0.9351 - val_acc: 0.5860\n",
      "Epoch 47/2000\n",
      "3100/3101 [============================>.] - ETA: 0s - loss: 0.9965 - acc: 0.5526\n",
      "Epoch 00047: val_loss did not improve from 0.93512\n",
      "3101/3101 [==============================] - 3s 892us/sample - loss: 0.9965 - acc: 0.5524 - val_loss: 0.9527 - val_acc: 0.5837\n",
      "Epoch 48/2000\n",
      "3100/3101 [============================>.] - ETA: 0s - loss: 1.0045 - acc: 0.5468\n",
      "Epoch 00048: val_loss did not improve from 0.93512\n",
      "3101/3101 [==============================] - 3s 886us/sample - loss: 1.0047 - acc: 0.5466 - val_loss: 1.0005 - val_acc: 0.5953\n",
      "Epoch 49/2000\n",
      "3100/3101 [============================>.] - ETA: 0s - loss: 1.0085 - acc: 0.5232\n",
      "Epoch 00049: val_loss did not improve from 0.93512\n",
      "3101/3101 [==============================] - 3s 897us/sample - loss: 1.0083 - acc: 0.5234 - val_loss: 0.9593 - val_acc: 0.5814\n",
      "Epoch 50/2000\n",
      "3100/3101 [============================>.] - ETA: 0s - loss: 0.9797 - acc: 0.5606\n",
      "Epoch 00050: val_loss did not improve from 0.93512\n",
      "3101/3101 [==============================] - 3s 924us/sample - loss: 0.9797 - acc: 0.5605 - val_loss: 1.0078 - val_acc: 0.5884\n",
      "Epoch 51/2000\n",
      "3100/3101 [============================>.] - ETA: 0s - loss: 1.0184 - acc: 0.5158\n",
      "Epoch 00051: val_loss did not improve from 0.93512\n",
      "3101/3101 [==============================] - 3s 927us/sample - loss: 1.0182 - acc: 0.5160 - val_loss: 0.9853 - val_acc: 0.5884\n",
      "Epoch 52/2000\n",
      "3100/3101 [============================>.] - ETA: 0s - loss: 0.9775 - acc: 0.5652\n",
      "Epoch 00052: val_loss did not improve from 0.93512\n",
      "3101/3101 [==============================] - 3s 933us/sample - loss: 0.9774 - acc: 0.5653 - val_loss: 0.9493 - val_acc: 0.5860\n",
      "Epoch 53/2000\n",
      "3100/3101 [============================>.] - ETA: 0s - loss: 0.9756 - acc: 0.5642\n",
      "Epoch 00053: val_loss did not improve from 0.93512\n",
      "3101/3101 [==============================] - 3s 901us/sample - loss: 0.9758 - acc: 0.5640 - val_loss: 0.9545 - val_acc: 0.5884\n",
      "Epoch 54/2000\n",
      "3100/3101 [============================>.] - ETA: 0s - loss: 1.0069 - acc: 0.5445\n",
      "Epoch 00054: val_loss did not improve from 0.93512\n",
      "3101/3101 [==============================] - 3s 908us/sample - loss: 1.0070 - acc: 0.5443 - val_loss: 0.9553 - val_acc: 0.5837\n",
      "Epoch 55/2000\n",
      "3100/3101 [============================>.] - ETA: 0s - loss: 0.9802 - acc: 0.5494\n",
      "Epoch 00055: val_loss did not improve from 0.93512\n",
      "3101/3101 [==============================] - 3s 907us/sample - loss: 0.9800 - acc: 0.5495 - val_loss: 0.9669 - val_acc: 0.5884\n",
      "Epoch 56/2000\n",
      "3100/3101 [============================>.] - ETA: 0s - loss: 0.9695 - acc: 0.5671\n",
      "Epoch 00056: val_loss did not improve from 0.93512\n",
      "3101/3101 [==============================] - 3s 899us/sample - loss: 0.9694 - acc: 0.5672 - val_loss: 0.9562 - val_acc: 0.5837\n",
      "Epoch 57/2000\n",
      "3100/3101 [============================>.] - ETA: 0s - loss: 0.9700 - acc: 0.5635\n",
      "Epoch 00057: val_loss did not improve from 0.93512\n",
      "3101/3101 [==============================] - 3s 902us/sample - loss: 0.9702 - acc: 0.5634 - val_loss: 0.9564 - val_acc: 0.5837\n",
      "Epoch 58/2000\n",
      "3100/3101 [============================>.] - ETA: 0s - loss: 0.9777 - acc: 0.5552\n",
      "Epoch 00058: val_loss did not improve from 0.93512\n",
      "3101/3101 [==============================] - 3s 892us/sample - loss: 0.9778 - acc: 0.5550 - val_loss: 0.9496 - val_acc: 0.5791\n",
      "Epoch 59/2000\n",
      "3100/3101 [============================>.] - ETA: 0s - loss: 0.9775 - acc: 0.5474\n",
      "Epoch 00059: val_loss did not improve from 0.93512\n",
      "3101/3101 [==============================] - 3s 891us/sample - loss: 0.9773 - acc: 0.5476 - val_loss: 0.9536 - val_acc: 0.5791\n",
      "Epoch 60/2000\n",
      "3100/3101 [============================>.] - ETA: 0s - loss: 0.9770 - acc: 0.5642\n",
      "Epoch 00060: val_loss did not improve from 0.93512\n",
      "3101/3101 [==============================] - 3s 904us/sample - loss: 0.9769 - acc: 0.5643 - val_loss: 0.9483 - val_acc: 0.5860\n",
      "Epoch 61/2000\n",
      "3100/3101 [============================>.] - ETA: 0s - loss: 0.9637 - acc: 0.5694\n",
      "Epoch 00061: val_loss did not improve from 0.93512\n",
      "3101/3101 [==============================] - 3s 892us/sample - loss: 0.9639 - acc: 0.5692 - val_loss: 0.9565 - val_acc: 0.5860\n",
      "Epoch 62/2000\n",
      "3100/3101 [============================>.] - ETA: 0s - loss: 0.9702 - acc: 0.5506\n",
      "Epoch 00062: val_loss did not improve from 0.93512\n",
      "3101/3101 [==============================] - 3s 905us/sample - loss: 0.9702 - acc: 0.5505 - val_loss: 0.9797 - val_acc: 0.5977\n",
      "Epoch 63/2000\n",
      "3100/3101 [============================>.] - ETA: 0s - loss: 0.9661 - acc: 0.5661\n",
      "Epoch 00063: val_loss did not improve from 0.93512\n",
      "3101/3101 [==============================] - 3s 931us/sample - loss: 0.9663 - acc: 0.5659 - val_loss: 0.9770 - val_acc: 0.6023\n",
      "Epoch 64/2000\n",
      "3100/3101 [============================>.] - ETA: 0s - loss: 0.9867 - acc: 0.5484\n",
      "Epoch 00064: val_loss did not improve from 0.93512\n",
      "3101/3101 [==============================] - 3s 891us/sample - loss: 0.9867 - acc: 0.5485 - val_loss: 0.9582 - val_acc: 0.5837\n",
      "Epoch 65/2000\n",
      "3100/3101 [============================>.] - ETA: 0s - loss: 0.9674 - acc: 0.5645\n",
      "Epoch 00065: val_loss did not improve from 0.93512\n",
      "3101/3101 [==============================] - 3s 882us/sample - loss: 0.9676 - acc: 0.5643 - val_loss: 0.9553 - val_acc: 0.6047\n",
      "Epoch 66/2000\n",
      "3100/3101 [============================>.] - ETA: 0s - loss: 0.9787 - acc: 0.5629\n",
      "Epoch 00066: val_loss did not improve from 0.93512\n",
      "3101/3101 [==============================] - 3s 890us/sample - loss: 0.9786 - acc: 0.5630 - val_loss: 0.9648 - val_acc: 0.5884\n",
      "Epoch 67/2000\n",
      "3100/3101 [============================>.] - ETA: 0s - loss: 0.9804 - acc: 0.5668\n",
      "Epoch 00067: val_loss did not improve from 0.93512\n",
      "3101/3101 [==============================] - 3s 883us/sample - loss: 0.9802 - acc: 0.5669 - val_loss: 0.9461 - val_acc: 0.5953\n",
      "Epoch 68/2000\n",
      "3100/3101 [============================>.] - ETA: 0s - loss: 0.9682 - acc: 0.5635\n",
      "Epoch 00068: val_loss did not improve from 0.93512\n",
      "3101/3101 [==============================] - 3s 895us/sample - loss: 0.9681 - acc: 0.5637 - val_loss: 0.9415 - val_acc: 0.5907\n",
      "Epoch 69/2000\n",
      "3100/3101 [============================>.] - ETA: 0s - loss: 0.9677 - acc: 0.5639\n",
      "Epoch 00069: val_loss did not improve from 0.93512\n",
      "3101/3101 [==============================] - 3s 905us/sample - loss: 0.9677 - acc: 0.5640 - val_loss: 0.9465 - val_acc: 0.5837\n",
      "Epoch 70/2000\n",
      "3100/3101 [============================>.] - ETA: 0s - loss: 0.9918 - acc: 0.5571\n",
      "Epoch 00070: val_loss did not improve from 0.93512\n",
      "3101/3101 [==============================] - 3s 894us/sample - loss: 0.9917 - acc: 0.5572 - val_loss: 0.9488 - val_acc: 0.5907\n",
      "Epoch 71/2000\n",
      "3100/3101 [============================>.] - ETA: 0s - loss: 0.9720 - acc: 0.5684\n",
      "Epoch 00071: val_loss did not improve from 0.93512\n",
      "3101/3101 [==============================] - 3s 912us/sample - loss: 0.9722 - acc: 0.5682 - val_loss: 0.9666 - val_acc: 0.5907\n",
      "Epoch 72/2000\n",
      "3100/3101 [============================>.] - ETA: 0s - loss: 0.9749 - acc: 0.5548\n",
      "Epoch 00072: val_loss did not improve from 0.93512\n",
      "3101/3101 [==============================] - 3s 901us/sample - loss: 0.9748 - acc: 0.5550 - val_loss: 0.9525 - val_acc: 0.5837\n",
      "Epoch 73/2000\n",
      "3100/3101 [============================>.] - ETA: 0s - loss: 0.9861 - acc: 0.5477\n",
      "Epoch 00073: val_loss did not improve from 0.93512\n",
      "3101/3101 [==============================] - 3s 888us/sample - loss: 0.9860 - acc: 0.5479 - val_loss: 0.9475 - val_acc: 0.5930\n",
      "Epoch 74/2000\n",
      "3100/3101 [============================>.] - ETA: 0s - loss: 0.9926 - acc: 0.5542\n",
      "Epoch 00074: val_loss did not improve from 0.93512\n",
      "3101/3101 [==============================] - 3s 873us/sample - loss: 0.9925 - acc: 0.5543 - val_loss: 0.9639 - val_acc: 0.5860\n",
      "Epoch 75/2000\n",
      "3100/3101 [============================>.] - ETA: 0s - loss: 0.9780 - acc: 0.5635\n",
      "Epoch 00075: val_loss did not improve from 0.93512\n",
      "3101/3101 [==============================] - 3s 884us/sample - loss: 0.9779 - acc: 0.5637 - val_loss: 0.9781 - val_acc: 0.5837\n",
      "Epoch 76/2000\n",
      "3100/3101 [============================>.] - ETA: 0s - loss: 0.9745 - acc: 0.5619\n",
      "Epoch 00076: val_loss did not improve from 0.93512\n",
      "3101/3101 [==============================] - 3s 892us/sample - loss: 0.9744 - acc: 0.5621 - val_loss: 0.9641 - val_acc: 0.5837\n",
      "Epoch 77/2000\n",
      "3100/3101 [============================>.] - ETA: 0s - loss: 0.9705 - acc: 0.5629\n",
      "Epoch 00077: val_loss did not improve from 0.93512\n",
      "3101/3101 [==============================] - 3s 905us/sample - loss: 0.9704 - acc: 0.5630 - val_loss: 0.9498 - val_acc: 0.5930\n",
      "Epoch 78/2000\n",
      "3100/3101 [============================>.] - ETA: 0s - loss: 0.9729 - acc: 0.5635\n",
      "Epoch 00078: val_loss did not improve from 0.93512\n",
      "3101/3101 [==============================] - 3s 909us/sample - loss: 0.9727 - acc: 0.5637 - val_loss: 0.9539 - val_acc: 0.5884\n",
      "Epoch 79/2000\n",
      "3100/3101 [============================>.] - ETA: 0s - loss: 0.9738 - acc: 0.5655\n",
      "Epoch 00079: val_loss did not improve from 0.93512\n",
      "3101/3101 [==============================] - 3s 884us/sample - loss: 0.9737 - acc: 0.5656 - val_loss: 0.9467 - val_acc: 0.5814\n",
      "Epoch 80/2000\n",
      "3100/3101 [============================>.] - ETA: 0s - loss: 0.9731 - acc: 0.5658\n",
      "Epoch 00080: val_loss did not improve from 0.93512\n",
      "3101/3101 [==============================] - 3s 891us/sample - loss: 0.9730 - acc: 0.5659 - val_loss: 0.9625 - val_acc: 0.5860\n",
      "Epoch 81/2000\n",
      "3100/3101 [============================>.] - ETA: 0s - loss: 0.9724 - acc: 0.5668\n",
      "Epoch 00081: val_loss did not improve from 0.93512\n",
      "3101/3101 [==============================] - 3s 854us/sample - loss: 0.9724 - acc: 0.5669 - val_loss: 0.9515 - val_acc: 0.5860\n",
      "Epoch 82/2000\n",
      "3100/3101 [============================>.] - ETA: 0s - loss: 0.9652 - acc: 0.5671\n",
      "Epoch 00082: val_loss did not improve from 0.93512\n",
      "3101/3101 [==============================] - 3s 910us/sample - loss: 0.9651 - acc: 0.5672 - val_loss: 0.9459 - val_acc: 0.5860\n",
      "Epoch 83/2000\n",
      "3100/3101 [============================>.] - ETA: 0s - loss: 0.9740 - acc: 0.5635\n",
      "Epoch 00083: val_loss did not improve from 0.93512\n",
      "3101/3101 [==============================] - 3s 897us/sample - loss: 0.9740 - acc: 0.5637 - val_loss: 0.9431 - val_acc: 0.5884\n",
      "Epoch 84/2000\n",
      "3100/3101 [============================>.] - ETA: 0s - loss: 0.9617 - acc: 0.5674\n",
      "Epoch 00084: val_loss did not improve from 0.93512\n",
      "3101/3101 [==============================] - 3s 919us/sample - loss: 0.9616 - acc: 0.5676 - val_loss: 0.9504 - val_acc: 0.5860\n",
      "Epoch 85/2000\n",
      "3100/3101 [============================>.] - ETA: 0s - loss: 0.9634 - acc: 0.5671\n",
      "Epoch 00085: val_loss did not improve from 0.93512\n",
      "3101/3101 [==============================] - 3s 927us/sample - loss: 0.9635 - acc: 0.5669 - val_loss: 0.9617 - val_acc: 0.5907\n",
      "Epoch 86/2000\n",
      "3100/3101 [============================>.] - ETA: 0s - loss: 0.9722 - acc: 0.5577\n",
      "Epoch 00086: val_loss did not improve from 0.93512\n",
      "3101/3101 [==============================] - 3s 925us/sample - loss: 0.9724 - acc: 0.5576 - val_loss: 0.9827 - val_acc: 0.5186\n",
      "Epoch 87/2000\n",
      "3100/3101 [============================>.] - ETA: 0s - loss: 0.9974 - acc: 0.5329\n",
      "Epoch 00087: val_loss did not improve from 0.93512\n",
      "3101/3101 [==============================] - 3s 922us/sample - loss: 0.9975 - acc: 0.5327 - val_loss: 0.9459 - val_acc: 0.5930\n",
      "Epoch 88/2000\n",
      "3100/3101 [============================>.] - ETA: 0s - loss: 0.9836 - acc: 0.5565\n",
      "Epoch 00088: val_loss did not improve from 0.93512\n",
      "3101/3101 [==============================] - 3s 911us/sample - loss: 0.9838 - acc: 0.5563 - val_loss: 0.9466 - val_acc: 0.5837\n",
      "Epoch 89/2000\n",
      "3100/3101 [============================>.] - ETA: 0s - loss: 0.9749 - acc: 0.5681\n",
      "Epoch 00089: val_loss did not improve from 0.93512\n",
      "3101/3101 [==============================] - 3s 915us/sample - loss: 0.9752 - acc: 0.5679 - val_loss: 0.9528 - val_acc: 0.5884\n",
      "Epoch 90/2000\n",
      "3100/3101 [============================>.] - ETA: 0s - loss: 0.9804 - acc: 0.5552\n",
      "Epoch 00090: val_loss did not improve from 0.93512\n",
      "3101/3101 [==============================] - 3s 885us/sample - loss: 0.9805 - acc: 0.5550 - val_loss: 1.0628 - val_acc: 0.4116\n",
      "Epoch 91/2000\n",
      "3100/3101 [============================>.] - ETA: 0s - loss: 1.0595 - acc: 0.5123\n",
      "Epoch 00091: val_loss did not improve from 0.93512\n",
      "3101/3101 [==============================] - 3s 907us/sample - loss: 1.0597 - acc: 0.5121 - val_loss: 0.9418 - val_acc: 0.5860\n",
      "Epoch 92/2000\n",
      "3100/3101 [============================>.] - ETA: 0s - loss: 0.9957 - acc: 0.5458\n",
      "Epoch 00092: val_loss did not improve from 0.93512\n",
      "3101/3101 [==============================] - 3s 906us/sample - loss: 0.9956 - acc: 0.5460 - val_loss: 0.9422 - val_acc: 0.5860\n",
      "Epoch 93/2000\n",
      "3100/3101 [============================>.] - ETA: 0s - loss: 0.9786 - acc: 0.5565\n",
      "Epoch 00093: val_loss did not improve from 0.93512\n",
      "3101/3101 [==============================] - 3s 883us/sample - loss: 0.9785 - acc: 0.5566 - val_loss: 0.9443 - val_acc: 0.5791\n",
      "Epoch 94/2000\n",
      "3100/3101 [============================>.] - ETA: 0s - loss: 0.9737 - acc: 0.5626\n",
      "Epoch 00094: val_loss did not improve from 0.93512\n",
      "3101/3101 [==============================] - 3s 903us/sample - loss: 0.9740 - acc: 0.5624 - val_loss: 0.9460 - val_acc: 0.5860\n",
      "Epoch 95/2000\n",
      "3100/3101 [============================>.] - ETA: 0s - loss: 0.9663 - acc: 0.5648\n",
      "Epoch 00095: val_loss did not improve from 0.93512\n",
      "3101/3101 [==============================] - 3s 892us/sample - loss: 0.9662 - acc: 0.5650 - val_loss: 0.9410 - val_acc: 0.5930\n",
      "Epoch 96/2000\n",
      "3100/3101 [============================>.] - ETA: 0s - loss: 0.9596 - acc: 0.5674\n",
      "Epoch 00096: val_loss did not improve from 0.93512\n",
      "3101/3101 [==============================] - 3s 909us/sample - loss: 0.9595 - acc: 0.5676 - val_loss: 0.9526 - val_acc: 0.5837\n",
      "Training time: 270.96392345428467 s\n",
      "776/776 [==============================] - 1s 790us/sample - loss: 0.9841 - acc: 0.5606\n",
      "\n",
      "Fold: 2\n",
      "\n",
      "Train on 3101 samples, validate on 430 samples\n",
      "Epoch 1/2000\n",
      "3100/3101 [============================>.] - ETA: 0s - loss: 0.9758 - acc: 0.5632\n",
      "Epoch 00001: val_loss improved from inf to 0.96964, saving model to /data/home/dorads/Documents/GitHub_Repositories/01_Maintained/NeuralODE/models/ChlorineConcentration/ChlorineConcentration-laguerre_2.hdf5\n",
      "3101/3101 [==============================] - 3s 921us/sample - loss: 0.9759 - acc: 0.5630 - val_loss: 0.9696 - val_acc: 0.5744\n",
      "Epoch 2/2000\n",
      "3100/3101 [============================>.] - ETA: 0s - loss: 0.9798 - acc: 0.5484\n",
      "Epoch 00002: val_loss did not improve from 0.96964\n",
      "3101/3101 [==============================] - 3s 898us/sample - loss: 0.9799 - acc: 0.5482 - val_loss: 0.9716 - val_acc: 0.5581\n",
      "Epoch 3/2000\n",
      "3100/3101 [============================>.] - ETA: 0s - loss: 0.9759 - acc: 0.5352\n",
      "Epoch 00003: val_loss improved from 0.96964 to 0.94028, saving model to /data/home/dorads/Documents/GitHub_Repositories/01_Maintained/NeuralODE/models/ChlorineConcentration/ChlorineConcentration-laguerre_2.hdf5\n",
      "3101/3101 [==============================] - 3s 911us/sample - loss: 0.9758 - acc: 0.5353 - val_loss: 0.9403 - val_acc: 0.6000\n",
      "Epoch 4/2000\n",
      "3100/3101 [============================>.] - ETA: 0s - loss: 0.9689 - acc: 0.5661\n",
      "Epoch 00004: val_loss improved from 0.94028 to 0.93780, saving model to /data/home/dorads/Documents/GitHub_Repositories/01_Maintained/NeuralODE/models/ChlorineConcentration/ChlorineConcentration-laguerre_2.hdf5\n",
      "3101/3101 [==============================] - 3s 908us/sample - loss: 0.9692 - acc: 0.5659 - val_loss: 0.9378 - val_acc: 0.5837\n",
      "Epoch 5/2000\n",
      "3100/3101 [============================>.] - ETA: 0s - loss: 0.9819 - acc: 0.5561\n",
      "Epoch 00005: val_loss did not improve from 0.93780\n",
      "3101/3101 [==============================] - 3s 891us/sample - loss: 0.9821 - acc: 0.5559 - val_loss: 0.9629 - val_acc: 0.5814\n",
      "Epoch 6/2000\n",
      "3100/3101 [============================>.] - ETA: 0s - loss: 0.9922 - acc: 0.5397\n",
      "Epoch 00006: val_loss did not improve from 0.93780\n",
      "3101/3101 [==============================] - 3s 899us/sample - loss: 0.9923 - acc: 0.5395 - val_loss: 0.9959 - val_acc: 0.5767\n",
      "Epoch 7/2000\n",
      "3100/3101 [============================>.] - ETA: 0s - loss: 0.9989 - acc: 0.5448\n",
      "Epoch 00007: val_loss did not improve from 0.93780\n",
      "3101/3101 [==============================] - 3s 892us/sample - loss: 0.9990 - acc: 0.5447 - val_loss: 0.9994 - val_acc: 0.5837\n",
      "Epoch 8/2000\n",
      "3100/3101 [============================>.] - ETA: 0s - loss: 0.9890 - acc: 0.5506\n",
      "Epoch 00008: val_loss did not improve from 0.93780\n",
      "3101/3101 [==============================] - 3s 888us/sample - loss: 0.9889 - acc: 0.5508 - val_loss: 0.9550 - val_acc: 0.5837\n",
      "Epoch 9/2000\n",
      "3100/3101 [============================>.] - ETA: 0s - loss: 0.9690 - acc: 0.5606\n",
      "Epoch 00009: val_loss did not improve from 0.93780\n",
      "3101/3101 [==============================] - 3s 898us/sample - loss: 0.9689 - acc: 0.5608 - val_loss: 0.9398 - val_acc: 0.5814\n",
      "Epoch 10/2000\n",
      "1300/3101 [===========>..................] - ETA: 1s - loss: 0.9846 - acc: 0.5608"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-c3170b12544e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m                        \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m                        \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m                        callbacks = callbacks)\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Training time: {time.time() - t} s\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/sd_dev_tf_1/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/miniconda3/envs/sd_dev_tf_1/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    673\u001b[0m         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m         \u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 675\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m    676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/miniconda3/envs/sd_dev_tf_1/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/sd_dev_tf_1/lib/python3.7/site-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3475\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3476\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3477\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3478\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[0;32m~/miniconda3/envs/sd_dev_tf_1/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if not os.path.exists(os.path.abspath(os.path.join('models', dataset))):\n",
    "    os.mkdir(os.path.abspath(os.path.join('models', dataset)))\n",
    "\n",
    "kf = KFold(n_splits = 5)\n",
    "sdk = StratifiedKFold(n_splits = 5, random_state = 42, shuffle = True)\n",
    "    \n",
    "n_fold = 1\n",
    "\n",
    "acc_per_fold = []\n",
    "epoch_per_fold = []\n",
    "loss_per_fold = []\n",
    "rec_per_fold = []\n",
    "prec_per_fold = []\n",
    "f1_per_fold = []\n",
    "\n",
    "for train, test in sdk.split(x_all, y_all):\n",
    "\n",
    "    file_path = os.path.abspath(os.path.join('models', dataset, f'{dataset}-laguerre_{n_fold}.hdf5'))\n",
    "\n",
    "    callbacks = [\n",
    "        ModelCheckpoint(filepath=file_path, monitor='val_loss', save_best_only = True, mode = 'min', verbose = 1), \n",
    "        EarlyStopping(monitor = 'val_loss', patience = 50, mode = 'min')]\n",
    "\n",
    "    t = time.time()\n",
    "    \n",
    "    print(f\"\\nFold: {n_fold}\\n\")\n",
    "    result = model.fit(x_all[train], \n",
    "                       to_categorical(y_all[train]),\n",
    "                       epochs = 2000, \n",
    "                       batch_size = 100, \n",
    "                       validation_data = (x_valid, to_categorical(y_valid)), \n",
    "                       callbacks = callbacks)\n",
    "\n",
    "    print(f\"Training time: {time.time() - t} s\")\n",
    "\n",
    "    df_results = pd.DataFrame(result.history)\n",
    "    df_results.to_csv(os.path.abspath(os.path.join('models', dataset, f'laguerre_results_{n_fold}.csv')))\n",
    "    \n",
    "    model.load_weights(file_path)\n",
    "    scores = model.evaluate(x_all[test], to_categorical(y_all[test]))\n",
    "    loss_per_fold.append(scores[0])\n",
    "    acc_per_fold.append(scores[1])\n",
    "    epoch_per_fold.append(np.argmin(result.history['val_loss']))\n",
    "    \n",
    "    # Computing predictions\n",
    "    y_pred_test = np.argmax(model.predict(x_all[test]), axis = 1)\n",
    "    \n",
    "    f1_per_fold.append(f1_score(y_all[test], y_pred_test, average = 'macro'))\n",
    "    rec_per_fold.append(recall_score(y_all[test], y_pred_test, average = 'macro'))\n",
    "    prec_per_fold.append(precision_score(y_all[test], y_pred_test, average = 'macro'))\n",
    "    \n",
    "    n_fold += 1\n",
    "    \n",
    "df_scores = pd.DataFrame(columns = ['F1', 'Loss', 'Accuracy', 'Precision', 'Recall'])\n",
    "df_scores['F1'] = f1_per_fold\n",
    "df_scores['Loss'] = loss_per_fold\n",
    "df_scores['Accuracy'] = acc_per_fold\n",
    "df_scores['Precision'] = prec_per_fold\n",
    "df_scores['Recall'] = rec_per_fold\n",
    "\n",
    "df_scores.to_csv(os.path.abspath(os.path.join('models', dataset, f'laguerre_results_k-Folds.csv')))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
